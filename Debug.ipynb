{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-27 16:08:23.098032: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/soulofshadow/anaconda3/lib/python3.11/site-packages/transformers/adapters/__init__.py:27: FutureWarning: The `adapter-transformers` package is deprecated and replaced by the `adapters` package. See https://docs.adapterhub.ml/transitioning.html.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from utils.log_helper import logger_init\n",
    "from modeling.modeling_llm import LLMGNP\n",
    "from utils.data_util import LLMGNP_DataLoader\n",
    "from utils import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"LLMGNP\")\n",
    "\n",
    "### Dataset\n",
    "parser.add_argument('--dataset', default='medqa', help='dataset name')\n",
    "parser.add_argument('--data_dir', default='data', type=str, help='Path to the data directory')\n",
    "parser.add_argument('--ent_emb_paths', default='umls/ent_emb_blbertL.npy', help='sources for entity embeddings')\n",
    "parser.add_argument('--pad_node', default=-1, type=int)\n",
    "#propmt\n",
    "parser.add_argument('--context_node', default=True, type=utils.bool_flag, nargs='?', const=True)\n",
    "parser.add_argument('--context_pad', default=-2, type=int)\n",
    "parser.add_argument('--add_edge_attr', default='no', choices=['no', 'solo_label', 'semantic'])\n",
    "parser.add_argument('--use_char_options_format', default=False, type=utils.bool_flag, nargs='?', const=True)\n",
    "parser.add_argument('--prompt_context', default=True, type=utils.bool_flag, nargs='?', const=True)\n",
    "parser.add_argument('--prompt_Lanswer', default=False, type=utils.bool_flag, nargs='?', const=True)\n",
    "#subgraph\n",
    "parser.add_argument('--num_subgraphs', default=4, type=int)\n",
    "parser.add_argument('--sub_graphs_choice', default='all', choices=['all', 'solo_question', 'solo_option'])\n",
    "parser.add_argument('--num_nodes', default=200, type=int)\n",
    "\n",
    "###Tune-P\n",
    "parser.add_argument('--learning_rate', default=1e-4, type=float, help='learning rate')\n",
    "parser.add_argument('--epochs', default=50, type=int, help='total number of training epochs to perform.')\n",
    "parser.add_argument('--batch_size', default=8, type=int)\n",
    "#Opt\n",
    "parser.add_argument('--optim', default='adamw', type=str)\n",
    "parser.add_argument('--lr_schedule', default='fixed', choices=['fixed', 'warmup_linear', 'warmup_constant'], help='learning rate scheduler')\n",
    "parser.add_argument('--warmup_ratio', type=float, default=0.1)\n",
    "parser.add_argument('--max_grad_norm', default=1.0, type=float, help='max grad norm (0 to disable)')\n",
    "parser.add_argument('--weight_decay', default=1e-2, type=float, help='l2 weight decay strength')\n",
    "\n",
    "#Task_weight\n",
    "parser.add_argument('--llm_task', type=float, default=1.0, help='Task weight for the LLM')\n",
    "parser.add_argument('--lp_task', type=float, default=0.1, help='Task weight for the LinkPred task')\n",
    "\n",
    "#-------Model-------\n",
    "###LLM Model\n",
    "parser.add_argument('--model_name',  default='google/flan-t5-small', help='encoder type')\n",
    "parser.add_argument('--max_seq_len', default=512, type=int)\n",
    "parser.add_argument('--max_tag_len', default=128, type=int)\n",
    "parser.add_argument('--llm_frozen', default=True, type=utils.bool_flag, nargs='?', const=True)\n",
    "#LLM decoder modity\n",
    "parser.add_argument('--xattn_heads', default=8, type=int)\n",
    "parser.add_argument('--xattn_after', default=True, type=utils.bool_flag, nargs='?', const=True)\n",
    "###GNP Model\n",
    "parser.add_argument('--add_gnp', default=True, type=utils.bool_flag, nargs='?', const=True)\n",
    "parser.add_argument('--cross_gnp', default=False, type=utils.bool_flag, nargs='?', const=True)\n",
    "parser.add_argument('--cross_gnp_num', default=5, type=int, help='attn_heads of the GNN layers')\n",
    "#initial_embed\n",
    "parser.add_argument('--random_ent_emb', default=False, type=utils.bool_flag, nargs='?', const=True, help='Whether to use randomly initialized learnable entity embeddings or not.')\n",
    "parser.add_argument('--freeze_ent_emb', default=True, type=utils.bool_flag, nargs='?', const=True, help='Whether to freeze the entity embedding layer.')\n",
    "#FFN\n",
    "parser.add_argument('--ffn_mult', default=4, type=int)\n",
    "#Gnn\n",
    "parser.add_argument('--gnn_layers', default=4, type=int, help='numbers of the GNN layers')\n",
    "parser.add_argument('--gnn_dim', default=1024, type=int, help='dimension of the GNN layers')\n",
    "parser.add_argument('--gnn_heads', default=2, type=int, help='attn_heads of the GNN layers')\n",
    "parser.add_argument('--gnn_norm', default=False, type=utils.bool_flag, nargs='?', const=True, help='encoder type')\n",
    "parser.add_argument('--gnn_residual', default='no', choices=['no', 'simple', 'linear'])\n",
    "parser.add_argument('--cross_gnp_choice', default=1, type=int, help='attn_heads of the GNN layers')\n",
    "#Self-Attention\n",
    "parser.add_argument('--self_attn_layers', default=1, type=int, help='numbers of the self-attention layers')\n",
    "parser.add_argument('--self_attn_heads', default=2, type=int, help='attn_heads of the self-attention layers')\n",
    "#Cross-Attention\n",
    "parser.add_argument('--cross_attn_layers', default=1, type=int, help='numbers of the CMP layers')\n",
    "parser.add_argument('--cross_attn_heads', default=2, type=int, help='attn_heads of the GNN layers')\n",
    "\n",
    "###Link Prediction\n",
    "parser.add_argument('--is_lp', default=True, type=utils.bool_flag, nargs='?', const=True)\n",
    "parser.add_argument('--link_drop_probability', type=float, default=0.1, help='To specify #target positive triples for LinkPred')\n",
    "parser.add_argument('--link_negative_sample_size', type=int, default=64, help='')\n",
    "parser.add_argument('--link_negative_adversarial_sampling', type=utils.bool_flag, default=True, help='')\n",
    "parser.add_argument('--link_negative_adversarial_sampling_temperature', type=float, default=1, help='')\n",
    "parser.add_argument('--link_regularizer_weight', type=float, default=0.01, help='')\n",
    "parser.add_argument('--scaled_distmult', type=utils.bool_flag, default=False, help='')\n",
    "#-------Model end-------\n",
    "\n",
    "#Activation\n",
    "parser.add_argument('--activation', default='gelu', type=str)\n",
    "parser.add_argument('--pre_norm', default=True, type=utils.bool_flag, nargs='?', const=True)\n",
    "parser.add_argument('--init_range', default=0.02, type=float, help='stddev when initializing with normal distribution')\n",
    "parser.add_argument('--dropout_emb', type=float, default=0.2, help='dropout for GNN layers')\n",
    "parser.add_argument('--dropout_ffn', type=float, default=0.2, help='dropout for GNN layers')\n",
    "parser.add_argument('--dropout_gnn', type=float, default=0.2, help='dropout for GNN layers')\n",
    "parser.add_argument('--dropout_self_attn', type=float, default=0.1, help='dropout for GNN layers')\n",
    "parser.add_argument('--dropout_cross_attn', type=float, default=0.1, help='dropout for GNN layers')\n",
    "\n",
    "#save and log\n",
    "parser.add_argument('--save_dir', default=f'./saved_models/', help='model output directory')\n",
    "parser.add_argument('--save_model', default=1, type=float, help=\"0: do not save model checkpoints. 1: save if best dev. 2: save always\")\n",
    "parser.add_argument('--load_model_path', default=None, help=\"The model checkpoint to load in the evaluation mode.\")\n",
    "\n",
    "#Test-Debug use\n",
    "parser.add_argument('--log_interval', default=10, type=int)\n",
    "parser.add_argument('--seed', default=1, type=int, help='random seed')\n",
    "parser.add_argument(\"--resume_checkpoint\", default=None, type=str,\n",
    "                    help=\"The checkpoint to resume training from.\")\n",
    "parser.add_argument('--mode', default='train', choices=['train', 'test'], help='run training or evaluation')\n",
    "\n",
    "args=parser.parse_args([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| num_concepts: 297927 |\n",
      "[2024-01-27 16:08:49] - INFO:  ### Right now config \n",
      "[2024-01-27 16:08:49] - INFO: ### dataset = medqa\n",
      "[2024-01-27 16:08:49] - INFO: ### data_dir = data\n",
      "[2024-01-27 16:08:49] - INFO: ### ent_emb_paths = umls/ent_emb_blbertL.npy\n",
      "[2024-01-27 16:08:49] - INFO: ### pad_node = -1\n",
      "[2024-01-27 16:08:49] - INFO: ### context_node = False\n",
      "[2024-01-27 16:08:49] - INFO: ### context_pad = -2\n",
      "[2024-01-27 16:08:49] - INFO: ### add_edge_attr = no\n",
      "[2024-01-27 16:08:49] - INFO: ### use_char_options_format = False\n",
      "[2024-01-27 16:08:49] - INFO: ### prompt_context = True\n",
      "[2024-01-27 16:08:49] - INFO: ### prompt_Lanswer = False\n",
      "[2024-01-27 16:08:49] - INFO: ### num_subgraphs = 5\n",
      "[2024-01-27 16:08:49] - INFO: ### sub_graphs_choice = all\n",
      "[2024-01-27 16:08:49] - INFO: ### num_nodes = 200\n",
      "[2024-01-27 16:08:49] - INFO: ### learning_rate = 0.0001\n",
      "[2024-01-27 16:08:49] - INFO: ### epochs = 50\n",
      "[2024-01-27 16:08:49] - INFO: ### batch_size = 8\n",
      "[2024-01-27 16:08:49] - INFO: ### optim = adamw\n",
      "[2024-01-27 16:08:49] - INFO: ### lr_schedule = fixed\n",
      "[2024-01-27 16:08:49] - INFO: ### warmup_ratio = 0.1\n",
      "[2024-01-27 16:08:49] - INFO: ### max_grad_norm = 1.0\n",
      "[2024-01-27 16:08:49] - INFO: ### weight_decay = 0.01\n",
      "[2024-01-27 16:08:49] - INFO: ### llm_task = 1.0\n",
      "[2024-01-27 16:08:50] - INFO: ### lp_task = 0.1\n",
      "[2024-01-27 16:08:50] - INFO: ### model_name = google/flan-t5-base\n",
      "[2024-01-27 16:08:50] - INFO: ### max_seq_len = 512\n",
      "[2024-01-27 16:08:50] - INFO: ### max_tag_len = 128\n",
      "[2024-01-27 16:08:50] - INFO: ### llm_frozen = True\n",
      "[2024-01-27 16:08:50] - INFO: ### xattn_heads = 8\n",
      "[2024-01-27 16:08:50] - INFO: ### xattn_after = True\n",
      "[2024-01-27 16:08:50] - INFO: ### add_gnp = True\n",
      "[2024-01-27 16:08:50] - INFO: ### cross_gnp = False\n",
      "[2024-01-27 16:08:50] - INFO: ### cross_gnp_num = 5\n",
      "[2024-01-27 16:08:50] - INFO: ### random_ent_emb = False\n",
      "[2024-01-27 16:08:50] - INFO: ### freeze_ent_emb = True\n",
      "[2024-01-27 16:08:50] - INFO: ### ffn_mult = 4\n",
      "[2024-01-27 16:08:50] - INFO: ### gnn_layers = 4\n",
      "[2024-01-27 16:08:50] - INFO: ### gnn_dim = 1024\n",
      "[2024-01-27 16:08:50] - INFO: ### gnn_heads = 2\n",
      "[2024-01-27 16:08:50] - INFO: ### gnn_norm = False\n",
      "[2024-01-27 16:08:50] - INFO: ### gnn_residual = linear\n",
      "[2024-01-27 16:08:50] - INFO: ### cross_gnp_choice = 1\n",
      "[2024-01-27 16:08:50] - INFO: ### self_attn_layers = 1\n",
      "[2024-01-27 16:08:50] - INFO: ### self_attn_heads = 2\n",
      "[2024-01-27 16:08:50] - INFO: ### cross_attn_layers = 1\n",
      "[2024-01-27 16:08:50] - INFO: ### cross_attn_heads = 2\n",
      "[2024-01-27 16:08:50] - INFO: ### is_lp = True\n",
      "[2024-01-27 16:08:50] - INFO: ### link_drop_probability = 0.1\n",
      "[2024-01-27 16:08:50] - INFO: ### link_negative_sample_size = 64\n",
      "[2024-01-27 16:08:50] - INFO: ### link_negative_adversarial_sampling = True\n",
      "[2024-01-27 16:08:50] - INFO: ### link_negative_adversarial_sampling_temperature = 1\n",
      "[2024-01-27 16:08:50] - INFO: ### link_regularizer_weight = 0.01\n",
      "[2024-01-27 16:08:50] - INFO: ### scaled_distmult = False\n",
      "[2024-01-27 16:08:50] - INFO: ### activation = gelu\n",
      "[2024-01-27 16:08:50] - INFO: ### pre_norm = True\n",
      "[2024-01-27 16:08:50] - INFO: ### init_range = 0.02\n",
      "[2024-01-27 16:08:50] - INFO: ### dropout_emb = 0.2\n",
      "[2024-01-27 16:08:50] - INFO: ### dropout_ffn = 0.2\n",
      "[2024-01-27 16:08:50] - INFO: ### dropout_gnn = 0.2\n",
      "[2024-01-27 16:08:50] - INFO: ### dropout_self_attn = 0.1\n",
      "[2024-01-27 16:08:50] - INFO: ### dropout_cross_attn = 0.1\n",
      "[2024-01-27 16:08:50] - INFO: ### save_dir = ./saved_models/\n",
      "[2024-01-27 16:08:50] - INFO: ### save_model = 1\n",
      "[2024-01-27 16:08:50] - INFO: ### load_model_path = None\n",
      "[2024-01-27 16:08:50] - INFO: ### log_interval = 10\n",
      "[2024-01-27 16:08:50] - INFO: ### seed = 1\n",
      "[2024-01-27 16:08:50] - INFO: ### resume_checkpoint = None\n",
      "[2024-01-27 16:08:50] - INFO: ### mode = test\n",
      "[2024-01-27 16:08:50] - INFO: ### project_dir = /Users/soulofshadow/Downloads/UMLS\n",
      "[2024-01-27 16:08:50] - INFO: ### dataset_dir = /Users/soulofshadow/Downloads/UMLS/data\n",
      "[2024-01-27 16:08:50] - INFO: ### dataset_name = medqa\n",
      "[2024-01-27 16:08:50] - INFO: ### t_train_path = /Users/soulofshadow/Downloads/UMLS/data/medqa/raw/train.jsonl\n",
      "[2024-01-27 16:08:50] - INFO: ### t_dev_path = /Users/soulofshadow/Downloads/UMLS/data/medqa/raw/dev.jsonl\n",
      "[2024-01-27 16:08:50] - INFO: ### t_test_path = /Users/soulofshadow/Downloads/UMLS/data/medqa/raw/test.jsonl\n",
      "[2024-01-27 16:08:50] - INFO: ### g_train_path = /Users/soulofshadow/Downloads/UMLS/data/medqa/subgraphed/train.jsonl\n",
      "[2024-01-27 16:08:50] - INFO: ### g_dev_path = /Users/soulofshadow/Downloads/UMLS/data/medqa/subgraphed/dev.jsonl\n",
      "[2024-01-27 16:08:50] - INFO: ### g_test_path = /Users/soulofshadow/Downloads/UMLS/data/medqa/subgraphed/test.jsonl\n",
      "[2024-01-27 16:08:50] - INFO: ### emb_ent_path = /Users/soulofshadow/Downloads/UMLS/data/umls/ent_emb_blbertL.npy\n",
      "[2024-01-27 16:08:50] - INFO: ### concept_num = 297927\n",
      "[2024-01-27 16:08:50] - INFO: ### concept_dim = 1024\n",
      "[2024-01-27 16:08:50] - INFO: ### pretrained_concept_emb = torch.Size([297927, 1024])\n",
      "[2024-01-27 16:08:50] - INFO: ### num_relations = 196\n",
      "[2024-01-27 16:08:50] - INFO: ### logs_save_dir = /Users/soulofshadow/Downloads/UMLS/logs\n",
      "[2024-01-27 16:08:50] - INFO: ### log_file_name = GNP_medqa\n"
     ]
    }
   ],
   "source": [
    "from train import ModelConfig\n",
    "import numpy as np\n",
    "\n",
    "args.dataset = 'medqa'\n",
    "args.sub_graphs_choice = 'all'\n",
    "\n",
    "args.mode = 'test'\n",
    "args.model_name=\"google/flan-t5-base\"\n",
    "args.llm_frozen=True\n",
    "\n",
    "args.add_gnp=True\n",
    "args.cross_gnp=False\n",
    "\n",
    "args.load_model_path = None\n",
    "args.context_node = False\n",
    "\n",
    "args.cross_gnp_choice = 1\n",
    "args.gnn_residual = 'linear'\n",
    "args.add_edge_attr = 'no'\n",
    "\n",
    "\n",
    " \n",
    "config = ModelConfig(vars(args))\n",
    "\n",
    "def get_device_and_set_seed(seed):\n",
    "    \"\"\" Set all seeds to make results reproducible \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    return device\n",
    "\n",
    "config.device = get_device_and_set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM...\n",
      "LLM Initialized\n",
      "Initializing GNP module...\n",
      " ### Initializing embedding for CustomizedEmbedding...\n",
      " ### CustomizedEmbedding Initialized\n",
      " ### Initializing w_relation for DistMultDecoder...\n",
      " ### DistMultDecoder Initialized\n",
      "GNP Initialized\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "model = LLMGNP(config)\n",
    "tokenizer = model.llm_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                                                 Param #\n",
       "===============================================================================================\n",
       "LLMGNP                                                                 --\n",
       "├─T5ForConditionalGeneration: 1-1                                      --\n",
       "│    └─ModuleDict: 2-1                                                 --\n",
       "│    └─Embedding: 2-2                                                  (24,674,304)\n",
       "│    └─T5Stack: 2-3                                                    24,674,304\n",
       "│    │    └─ModuleDict: 3-1                                            --\n",
       "│    │    └─Embedding: 3-2                                             (recursive)\n",
       "│    │    └─ModuleList: 3-3                                            (84,953,472)\n",
       "│    │    └─T5LayerNorm: 3-4                                           (768)\n",
       "│    │    └─Dropout: 3-5                                               --\n",
       "│    └─T5Stack: 2-4                                                    24,674,304\n",
       "│    │    └─ModuleDict: 3-6                                            --\n",
       "│    │    └─Embedding: 3-7                                             (recursive)\n",
       "│    │    └─ModuleList: 3-8                                            (113,274,240)\n",
       "│    │    └─T5LayerNorm: 3-9                                           (768)\n",
       "│    │    └─Dropout: 3-10                                              --\n",
       "│    └─Linear: 2-5                                                     (24,674,304)\n",
       "│    └─PrefixTuningPool: 2-6                                           --\n",
       "│    │    └─ModuleDict: 3-11                                           --\n",
       "├─GNP: 1-2                                                             --\n",
       "│    └─CustomizedEmbedding: 2-7                                        --\n",
       "│    │    └─Embedding: 3-12                                            (305,078,272)\n",
       "│    └─Dropout: 2-8                                                    --\n",
       "│    └─GNNBlock: 2-9                                                   --\n",
       "│    │    └─FFNLayer: 3-13                                             5,505,024\n",
       "│    │    └─GATNet: 3-14                                               9,454,592\n",
       "│    │    └─SelfAttnModule: 3-15                                       4,196,352\n",
       "│    │    └─CrossModalityAttnModule: 3-16                              4,196,352\n",
       "│    └─AveragePoolingLayer: 2-10                                       --\n",
       "│    └─FFNLayer: 2-11                                                  --\n",
       "│    │    └─Sequential: 3-17                                           7,340,032\n",
       "│    └─DistMultDecoder: 2-12                                           200,704\n",
       "===============================================================================================\n",
       "Total params: 632,897,792\n",
       "Trainable params: 30,893,056\n",
       "Non-trainable params: 602,004,736\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "dataset = LLMGNP_DataLoader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subgraph\n",
    "data_g = []\n",
    "with open('./data/medqa/subgraphed/train.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line)\n",
    "        data_g.append(json_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "for data in data_g:\n",
    "    data[:] = list(chain(*data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'nodes':[], 'edges': [], 'edge_types':[]}\n",
    "\n",
    "for data in data_g:\n",
    "    for each in data:\n",
    "        if not each['edges']:\n",
    "            each.update(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.jsonl', 'w') as fout:\n",
    "    for dic in data_g:\n",
    "        fout.write(json.dumps(dic) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug each batch\n",
    "batch = dataset.train_dataset[0]\n",
    "lminputs, lmlabels, gnndata = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'yes']\n",
      "tensor(1.3692)\n"
     ]
    }
   ],
   "source": [
    "#LLM test\n",
    "labels = lmlabels['input_ids']\n",
    "labels[labels == model.llm_tokenizer.pad_token_id] = -100\n",
    "labels_masks = lmlabels['attention_mask']\n",
    "\n",
    "output = model.llm_model(input_ids = lminputs['input_ids'],\n",
    "                                    attention_mask = lminputs['attention_mask'],\n",
    "                                    labels =labels,\n",
    "                                    decoder_attention_mask=labels_masks)\n",
    "preds = F.softmax(output.logits, dim=-1).argmax(dim=-1)\n",
    "print(tokenizer.batch_decode(preds, skip_special_tokens=True))\n",
    "print(output.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes']\n",
      "tensor(1.4679, grad_fn=<NllLossBackward0>)\n",
      "(tensor(0.6969, grad_fn=<AddBackward0>), tensor(0.6934, grad_fn=<NegBackward0>), tensor(0.7004, grad_fn=<NegBackward0>))\n"
     ]
    }
   ],
   "source": [
    "#Model test\n",
    "logits, lm_loss, lp_loss = model(batch, 'train')\n",
    "preds = F.softmax(logits, dim=-1).argmax(dim=-1)\n",
    "print(tokenizer.batch_decode(preds, skip_special_tokens=True))\n",
    "print(lm_loss)\n",
    "print(lp_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    # print(i)\n",
    "\n",
    "    if i != 5 - 1 :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_g = []\n",
    "with open('/Users/soulofshadow/Downloads/UMLS/data/bioasq/subgraphed/test.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line)\n",
    "        data_g.append(json_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = len(H.edge_index[0])\n",
    "positions = torch.arange(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = None\n",
    "for i in range(len(data_g)):\n",
    "    one_sample_graphs = []\n",
    "    for j in range(len(data_g[0])):\n",
    "        sample = data_g[i][j]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = sample['nodes'][:200 - 1]\n",
    "edge_list = []\n",
    "edge_attr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes[1:]:\n",
    "    edge_list.append([nodes[0], node])\n",
    "    edge_attr.append(-1)\n",
    "for index, [a, b] in enumerate(sample['edges']):\n",
    "    if a in nodes and b in nodes:\n",
    "        edge_list.append([a, b])\n",
    "        edge_attr.append(sample['edge_types'][index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_nodes = [s for s, t in edge_list]\n",
    "target_nodes = [t for s, t in edge_list]\n",
    "node_id_to_index = {node_id: idx for idx, node_id in enumerate(nodes)}\n",
    "edge_index = torch.tensor([\n",
    "    [node_id_to_index[s] for s in source_nodes],\n",
    "    [node_id_to_index[t] for t in target_nodes]\n",
    "], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = torch.tensor(nodes, dtype=torch.long)\n",
    "if nodes.size(0) < 200:\n",
    "    pad_length = 200 - nodes.size(0)\n",
    "    nodes = torch.cat([nodes, torch.tensor([-1] * pad_length, dtype=torch.long)], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 75])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr = [x-98 if x > (98 - 1) else x for x in edge_attr]\n",
    "edge_attr = [new_dict[x] for x in edge_attr]\n",
    "edge_attr = tokenizer(edge_attr, return_tensors='pt', padding=True, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([75, 12])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attr['input_ids'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNP debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, all_hidden_states = model.get_embedding(lminputs, gnndata)\n",
    "T_mask = lminputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = T.size(0) * config.num_subgraphs #batch_size * num_subgraphs\n",
    "        \n",
    "#init H + GNN\n",
    "H = model.GNP.batch_choice_of_graph(gnndata)\n",
    "H.x = model.GNP.ent_embed_init(H.x)\n",
    "H.x = model.GNP.dropout_emb(H.x)\n",
    "H_mask = H.mask\n",
    "H_mask = H_mask.view(batch_num, config.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = len(H.edge_index[0])\n",
    "positions = torch.arange(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_nodes = set()\n",
    "for item in H.edge_index.flatten():\n",
    "    effective_nodes.add(item.item()) \n",
    "effective_nodes = list(effective_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[effective_nodes[idx] for idx in row] for row in neg_nodes_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 60, 287, 101, 422, 651, 308, 116, 293, 159, 213],\n",
      "        [ 85, 151, 192, 402, 715, 733, 753, 532, 292, 691],\n",
      "        [262, 153, 720, 868, 902, 610, 496, 592, 461, 535],\n",
      "        [ 58, 917, 650, 172, 862, 467, 910, 879, 545, 746],\n",
      "        [409, 894,   9, 517, 310, 318, 175, 775, 313, 687],\n",
      "        [283, 184, 506, 325, 801, 263, 801, 547, 216,  28],\n",
      "        [274, 802, 280, 745, 932,  15, 189, 867, 785, 144],\n",
      "        [730, 639, 260, 413, 517, 706, 847, 255, 307, 356],\n",
      "        [890, 693, 663, 789, 918, 746, 133,  84, 507, 902],\n",
      "        [471, 457, 275, 634, 376, 327, 378, 848, 265, 627]])\n"
     ]
    }
   ],
   "source": [
    "neg_nodes_indices = torch.randint(0, len(effective_nodes), (10, 10)) \n",
    "print(neg_nodes_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[60, 287, 101, 485, 810, 308, 116, 293, 159, 213],\n",
       " [85, 151, 192, 465, 874, 1005, 1025, 595, 292, 850],\n",
       " [262, 153, 879, 1140, 1225, 673, 559, 655, 524, 598],\n",
       " [58, 1240, 809, 172, 1134, 530, 1233, 1202, 608, 1018],\n",
       " [472, 1217, 9, 580, 310, 318, 175, 1047, 313, 846],\n",
       " [283, 184, 569, 325, 1073, 263, 1073, 610, 216, 28],\n",
       " [274, 1074, 280, 1017, 1411, 15, 189, 1139, 1057, 144],\n",
       " [1002, 702, 260, 476, 580, 865, 1119, 255, 307, 419],\n",
       " [1213, 852, 822, 1061, 1241, 1018, 133, 84, 570, 1225],\n",
       " [534, 520, 275, 697, 439, 327, 441, 1120, 265, 690]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[effective_nodes[idx] for idx in row] for row in neg_nodes_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 1412, 1412, 1414],\n",
       "        [   1,    2,    3,  ..., 1414, 1418, 1417]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1600, 1024])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.x[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_special = H.edge_lp[H.edge_lp == config.num_relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_special = H.edge_lp == config.num_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1419"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions = positions[H.edge_lp == config.num_relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_input = H.x.view(batch_num, config.num_nodes, -1)\n",
    "\n",
    "for gnn_block, lm_hidden_states in zip(model.GNP.gnn_blocks, all_hidden_states[-len(model.GNP.gnn_blocks):]):\n",
    "    H.x = gnn_block(H, lm_hidden_states, T_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_after_cross = H.x.view(batch_num, config.num_nodes, -1)\n",
    "nodes_after_cross = model.GNP.activation_gat(nodes_after_cross)\n",
    "nodes_after_cross = model.GNP.activation_residual(model.GNP.Vh(nodes_input) + model.GNP.Vx(nodes_after_cross))\n",
    "#\n",
    "H_node_final = nodes_after_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 #[0,1,2,3,4]\n",
    "gnn_block = model.GNP.gnn_blocks[i]\n",
    "lm_hidden_states = all_hidden_states[-(5-i)]\n",
    "\n",
    "origin_size = H.x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  1.1256,  0.3548,  ..., -0.8073, -1.0626,  1.2089],\n",
       "        [-0.0791,  0.0000, -0.0707,  ..., -0.0000, -0.0000,  1.1810],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0170,  0.0224,  0.0295,  ...,  0.0125, -0.0368,  0.0369],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0.6052, 0.5988, 0.8457,  ..., 0.5642, 1.1095, 0.1331],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(2.9801, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "TT = torch.repeat_interleave(lm_hidden_states, repeats=config.num_subgraphs, dim=0)\n",
    "TT_mask = torch.repeat_interleave(T_mask, repeats=config.num_subgraphs, dim=0)\n",
    "\n",
    "HH = gnn_block.GNN(H)\n",
    "HH = HH.view(TT.size(0), config.num_nodes, -1)\n",
    "print(HH[0][0])\n",
    "print(HH[0][1])\n",
    "print(torch.max(HH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  67.6272, -126.7188,   61.7799,  ...,  424.8896, -303.9418,\n",
      "         573.8397], grad_fn=<SelectBackward0>)\n",
      "tensor([0.6052, 0.5988, 0.8457,  ..., 0.5642, 1.1095, 0.1331],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(731.6631, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "context_node_gnn_feats = HH[:, 0, :].clone() # [bs, node_dim]\n",
    "context_node_gnn_feats = gnn_block.norm(context_node_gnn_feats)\n",
    "context_node_lm_feats = gnn_block.cross_attn(context_node_gnn_feats.unsqueeze(1), \n",
    "                                        TT, \n",
    "                                        TT_mask)\n",
    "context_node_lm_feats = context_node_lm_feats.squeeze(1) # [bs, node_dim]\n",
    "context_node_feats = torch.cat([context_node_lm_feats, context_node_gnn_feats], dim=1)\n",
    "context_node_feats = gnn_block.merge_ffn(context_node_feats)\n",
    "\n",
    "# residual link\n",
    "context_node_feats = context_node_feats + context_node_gnn_feats\n",
    "HH[:, 0, :] = context_node_feats\n",
    "\n",
    "print(HH[0][0])\n",
    "print(HH[0][1])\n",
    "print(torch.max(HH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.x = HH.view(origin_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1 #[0,1,2,3,4]\n",
    "gnn_block = model.GNP.gnn_blocks[i]\n",
    "lm_hidden_states = all_hidden_states[-(5-i)]\n",
    "\n",
    "origin_size = H.x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-328.4462, -144.5006, -417.6876,  ..., -333.5429,  113.1736,\n",
      "        -143.5552], grad_fn=<SelectBackward0>)\n",
      "tensor([-0.1583, -0.1150, -0.0586,  ..., -0.2677,  0.2841, -0.4118],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(928.3207, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "TT = torch.repeat_interleave(lm_hidden_states, repeats=config.num_subgraphs, dim=0)\n",
    "TT_mask = torch.repeat_interleave(T_mask, repeats=config.num_subgraphs, dim=0)\n",
    "\n",
    "HH = gnn_block.GNN(H)\n",
    "HH = HH.view(TT.size(0), config.num_nodes, -1)\n",
    "print(HH[0][0])\n",
    "print(HH[0][2])\n",
    "print(torch.max(HH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0454, -0.0207, -0.1212,  ..., -0.2516, -0.7245,  1.0667],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(HH[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 1419, 1419, 1419],\n",
       "        [   1,    2,    3,  ..., 1420, 1421, 1426]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  4.5510,   8.2308, -18.0366,  ...,  -4.0156,  -2.6247,  12.5429],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([-0.3618, -0.2496,  0.4299,  ..., -0.4742, -0.0483, -0.2053],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(863.4166, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "context_node_gnn_feats = HH[:, 0, :].clone() # [bs, node_dim]\n",
    "context_node_gnn_feats = gnn_block.norm(context_node_gnn_feats)\n",
    "context_node_lm_feats = gnn_block.cross_attn(context_node_gnn_feats.unsqueeze(1), \n",
    "                                        TT, \n",
    "                                        TT_mask)\n",
    "context_node_lm_feats = context_node_lm_feats.squeeze(1) # [bs, node_dim]\n",
    "context_node_feats = torch.cat([context_node_lm_feats, context_node_gnn_feats], dim=1)\n",
    "context_node_feats = gnn_block.merge_ffn(context_node_feats)\n",
    "\n",
    "# residual link\n",
    "context_node_feats = context_node_feats + context_node_gnn_feats\n",
    "HH[:, 0, :] = context_node_feats\n",
    "\n",
    "print(HH[0][0])\n",
    "print(HH[0][1])\n",
    "print(torch.max(HH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.x = HH.view(origin_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-14.5090, -10.2454,  -5.1613,  ...,  35.2490, -18.4673,   9.3669],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.0727, -0.2229,  0.0707,  ...,  0.0817, -0.4731,  0.5370],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(1096.0328, grad_fn=<MaxBackward1>)\n",
      "tensor([-584.8746,  217.7348,  771.8218,  ...,  262.2571, -127.7205,\n",
      "         456.6053], grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.0727, -0.2229,  0.0707,  ...,  0.0817, -0.4731,  0.5370],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(1702.2756, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "i = 3 #[0,1,2,3,4]\n",
    "gnn_block = model.GNP.gnn_blocks[i]\n",
    "lm_hidden_states = all_hidden_states[-(5-i)]\n",
    "\n",
    "origin_size = H.x.size(0)\n",
    "\n",
    "TT = torch.repeat_interleave(lm_hidden_states, repeats=config.num_subgraphs, dim=0)\n",
    "TT_mask = torch.repeat_interleave(T_mask, repeats=config.num_subgraphs, dim=0)\n",
    "\n",
    "HH = gnn_block.GNN(H)\n",
    "HH = HH.view(TT.size(0), config.num_nodes, -1)\n",
    "print(HH[0][0])\n",
    "print(HH[0][1])\n",
    "print(torch.max(HH))\n",
    "\n",
    "context_node_gnn_feats = HH[:, 0, :].clone() # [bs, node_dim]\n",
    "context_node_gnn_feats = gnn_block.norm(context_node_gnn_feats)\n",
    "context_node_lm_feats = gnn_block.cross_attn(context_node_gnn_feats.unsqueeze(1), \n",
    "                                        TT, \n",
    "                                        TT_mask)\n",
    "context_node_lm_feats = context_node_lm_feats.squeeze(1) # [bs, node_dim]\n",
    "context_node_feats = torch.cat([context_node_lm_feats, context_node_gnn_feats], dim=1)\n",
    "context_node_feats = gnn_block.merge_ffn(context_node_feats)\n",
    "\n",
    "# residual link\n",
    "context_node_feats = context_node_feats + context_node_gnn_feats\n",
    "HH[:, 0, :] = context_node_feats\n",
    "\n",
    "print(HH[0][0])\n",
    "print(HH[0][1])\n",
    "print(torch.max(HH))\n",
    "\n",
    "H.x = HH.view(origin_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -472.9886,  1065.9122,    -2.3353,  ...,  1392.9152,   614.6643,\n",
      "        -1501.3160], grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.4715, -1.1434,  1.1304,  ..., -2.5475,  0.1591, -0.5176],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(2666.8748, grad_fn=<MaxBackward1>)\n",
      "tensor([ 383.9848,  640.9866,  133.9565,  ..., -633.9208, -509.8645,\n",
      "           3.7313], grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.4715, -1.1434,  1.1304,  ..., -2.5475,  0.1591, -0.5176],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(1685.3871, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "i = 3 #[0,1,2,3,4]\n",
    "gnn_block = model.GNP.gnn_blocks[i]\n",
    "lm_hidden_states = all_hidden_states[-(5-i)]\n",
    "\n",
    "origin_size = H.x.size(0)\n",
    "\n",
    "TT = torch.repeat_interleave(lm_hidden_states, repeats=config.num_subgraphs, dim=0)\n",
    "TT_mask = torch.repeat_interleave(T_mask, repeats=config.num_subgraphs, dim=0)\n",
    "\n",
    "HH = gnn_block.GNN(H)\n",
    "HH = HH.view(TT.size(0), config.num_nodes, -1)\n",
    "print(HH[0][0])\n",
    "print(HH[0][1])\n",
    "print(torch.max(HH))\n",
    "\n",
    "context_node_gnn_feats = HH[:, 0, :].clone() # [bs, node_dim]\n",
    "context_node_gnn_feats = gnn_block.norm(context_node_gnn_feats)\n",
    "context_node_lm_feats = gnn_block.cross_attn(context_node_gnn_feats.unsqueeze(1), \n",
    "                                        TT, \n",
    "                                        TT_mask)\n",
    "context_node_lm_feats = context_node_lm_feats.squeeze(1) # [bs, node_dim]\n",
    "context_node_feats = torch.cat([context_node_lm_feats, context_node_gnn_feats], dim=1)\n",
    "context_node_feats = gnn_block.merge_ffn(context_node_feats)\n",
    "\n",
    "# residual link\n",
    "context_node_feats = context_node_feats + context_node_gnn_feats\n",
    "HH[:, 0, :] = context_node_feats\n",
    "\n",
    "print(HH[0][0])\n",
    "print(HH[0][1])\n",
    "print(torch.max(HH))\n",
    "\n",
    "H.x = HH.view(origin_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "i = 4 #[0,1,2,3,4]\n",
    "gnn_block = model.GNP.gnn_blocks[i]\n",
    "lm_hidden_states = all_hidden_states[-(5-i)]\n",
    "\n",
    "origin_size = H.x.size(0)\n",
    "\n",
    "TT = torch.repeat_interleave(lm_hidden_states, repeats=config.num_subgraphs, dim=0)\n",
    "TT_mask = torch.repeat_interleave(T_mask, repeats=config.num_subgraphs, dim=0)\n",
    "\n",
    "HH = gnn_block.GNN(H)\n",
    "HH = HH.view(TT.size(0), config.num_nodes, -1)\n",
    "print(HH[0][0])\n",
    "print(HH[0][1])\n",
    "print(torch.max(HH))\n",
    "\n",
    "context_node_gnn_feats = HH[:, 0, :].clone() # [bs, node_dim]\n",
    "context_node_gnn_feats = gnn_block.norm(context_node_gnn_feats)\n",
    "context_node_lm_feats = gnn_block.cross_attn(context_node_gnn_feats.unsqueeze(1), \n",
    "                                        TT, \n",
    "                                        TT_mask)\n",
    "context_node_lm_feats = context_node_lm_feats.squeeze(1) # [bs, node_dim]\n",
    "context_node_feats = torch.cat([context_node_lm_feats, context_node_gnn_feats], dim=1)\n",
    "context_node_feats = gnn_block.merge_ffn(context_node_feats)\n",
    "\n",
    "# residual link\n",
    "context_node_feats = context_node_feats + context_node_gnn_feats\n",
    "HH[:, 0, :] = context_node_feats\n",
    "\n",
    "print(HH[0][0])\n",
    "print(HH[0][1])\n",
    "print(torch.max(HH))\n",
    "\n",
    "H.x = HH.view(origin_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
