{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/soulofshadow/Downloads/Project'\n",
    "\n",
    "os.chdir(path)\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get from hg pubmed abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/soulofshadow/.cache/huggingface/datasets/ywchoi___parquet/ywchoi--pubmed_abstract_0-3c14f02f0075d77a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58292ddbf1ad4e0e8b25a4aa5488abfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = 'ywchoi/pubmed_abstract_0'\n",
    "abstracts = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = abstracts['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "try to split a whole abstract to shorter sentence\n",
    "'''\n",
    "\n",
    "# def split_text_into_items(text, num_sentences_per_item=3):\n",
    "#     # Use regular expressions to divide text into sentences\n",
    "#     sentences = re.split(r'(?<=[.!?])\\s', text)\n",
    "\n",
    "#     items = []\n",
    "    \n",
    "#     # Organize sentences into one form for every two or three sentences.\n",
    "#     for i in range(0, len(sentences), num_sentences_per_item):\n",
    "#         item = ' '.join(sentences[i:i + num_sentences_per_item])\n",
    "#         items.append(item)\n",
    "    \n",
    "#     return items\n",
    "\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for abstract in abstracts:\n",
    "#     result = split_text_into_items(abstract, num_sentences_per_item=3)\n",
    "#     results.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An impaired heart rate recovery (HRR) has been associated with increased risk of cardiovascular events, cardiovascular, and all-cause mortality. However, the diagnostic ability of HRR for the presence and severity of coronary artery disease (CAD) has not been clearly elucidated. Our aim was to investigate the relationship between HRR and the SYNTAX (SYNergy between percutaneous coronary intervention with TAXus and cardiac surgery) score in patients with stable CAD (SCAD). A total of 406 patients with an abnormal treadmill exercise test and ≥50% coronary stenosis on coronary angiography were included. The HRR was calculated by subtracting the HR in the first minute of the recovery period from the maximum HR during exercise. The SYNTAX score ≥23 was accepted as high. Correlation of HRR with SYNTAX score and independent predictors of high SYNTAX score were determined. A high SYNTAX score was present in 172 (42%) patients. Mean HRR was lower in patients with a high SYNTAX score (9.8 ± 4.5 vs. 21.3 ± 9, p < 0.001). The SYNTAX score was negatively correlated with HRR (r: -0.580, p < 0.001). In multivariate logistic regression analysis, peripheral arterial disease (OR: 13.3; 95% CI: 3.120-34.520; p < 0.001), decreased HRR (OR: 0.780; 95% CI: 0.674-0.902; p = 0.001), peak systolic blood pressure (OR: 1.054; 95% CI: 1.023-1.087; p = 0.001), and peak HR (OR: 0.950; 95% CI: 0.923-0.977; p < 0.001) were found to be independent predictors of a high SYNTAX score. Our results showed that HRR is significantly correlated with the SYNTAX score, and a decreased HRR is an independent predictor of a high SYNTAX score in patients with SCAD.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MetaMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting skrmedpostctl: \n",
      "started.\n",
      "Starting wsdserverctl: \n",
      "started.\n",
      "loading properties file /Users/soulofshadow/Downloads/Project/public_mm/WSD_Server/config/disambServer.cfg\n"
     ]
    }
   ],
   "source": [
    "from pymetamap import MetaMap\n",
    "from time import sleep\n",
    "# Setup UMLS Server\n",
    "\n",
    "metamap_base_dir = path + '/public_mm'\n",
    "metamap_bin_dir = '/bin/metamap18'\n",
    "metamap_pos_server_dir = '/bin/skrmedpostctl'\n",
    "metamap_wsd_server_dir = '/bin/wsdserverctl'\n",
    "\n",
    "# Start servers\n",
    "os.system(metamap_base_dir + metamap_pos_server_dir + ' start') # Part of speech tagger\n",
    "os.system(metamap_base_dir + metamap_wsd_server_dir + ' start') # Word sense disambiguation \n",
    "\n",
    "# Sleep a bit to give time for these servers to start up\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MetaMap.get_instance(metamap_base_dir + metamap_bin_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_of_interest = ['preferred_name', 'cui', 'semtypes', 'pos_info']\n",
    "\n",
    "def get_enti_from_mm(concepts, klist):\n",
    "    entities = []\n",
    "    for concept in concepts:\n",
    "        if concept.__class__.__name__ == 'ConceptMMI':\n",
    "            conc_dict = concept._asdict()\n",
    "            conc_list = [conc_dict.get(kk) for kk in klist]\n",
    "            key_value_pairs = zip(klist, conc_list)\n",
    "            my_dict = {key: value for key, value in key_value_pairs}\n",
    "            entities.append(my_dict)\n",
    "\n",
    "    return entities\n",
    "\n",
    "def remove_non_ascii(input_string):\n",
    "    # Use a regular expression to match non-ASCII characters and replace them with an empty string\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path + '/data/build_tri/text_to_entity.csv'\n",
    "\n",
    "def creat_csv(path):\n",
    "    path  = path\n",
    "    with open(path,'a+') as f:\n",
    "        csv_write = csv.writer(f)\n",
    "        data_row = [\"text\",\"entity\"]\n",
    "        csv_write.writerow(data_row)\n",
    "\n",
    "def write_csv(data, path):\n",
    "    path  = path\n",
    "    with open(path,'a+') as f:\n",
    "        csv_write = csv.writer(f)\n",
    "        csv_write.writerow(data)\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    creat_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220240\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "forcount = pd.read_csv(file_path)\n",
    "\n",
    "print(len(forcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "results = abstracts[250000:]\n",
    "for sentence in results:\n",
    "    count += 1\n",
    "    sample = [remove_non_ascii(sentence)]\n",
    "    cons, errs = mm.extract_concepts(sample,\n",
    "                                word_sense_disambiguation = True,\n",
    "                                strict_model=True,\n",
    "                                composite_phrase = 1,\n",
    "                                prune = 30)\n",
    "\n",
    "    get_ = get_enti_from_mm(cons, keys_of_interest)\n",
    "    if get_:\n",
    "        write_csv([sentence, get_], file_path)\n",
    "\n",
    "    if count % 1000 == 0:\n",
    "        print(\"Handled: \", count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
