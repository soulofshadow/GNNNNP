{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["8gkmecp8wyVT"],"authorship_tag":"ABX9TyOgrvKIwSh8C5TFDJUHtcF+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### General"],"metadata":{"id":"8gkmecp8wyVT"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6tq2yCuorIb3","executionInfo":{"status":"ok","timestamp":1694099061367,"user_tz":-120,"elapsed":21727,"user":{"displayName":"Joseph","userId":"02183415066300269879"}},"outputId":"363b1501-a697-4f9b-b94f-0bf7fb0995e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","# Here is the path of the root dir of this folder in your google drive\n","path=\"/content/drive/My Drive/Project\"\n","\n","import os\n","import sys\n","os.chdir(path)\n","sys.path.append(path)"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import json\n","import random\n","import copy\n","import h5py\n","import math\n","from tqdm import tqdm\n","tqdm.pandas()"],"metadata":{"id":"pi58bzJexC5r","executionInfo":{"status":"ok","timestamp":1694099061726,"user_tz":-120,"elapsed":362,"user":{"displayName":"Joseph","userId":"02183415066300269879"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vSiYJQlwvm4H","executionInfo":{"status":"ok","timestamp":1694099066722,"user_tz":-120,"elapsed":4998,"user":{"displayName":"Joseph","userId":"02183415066300269879"}},"outputId":"ea93c0b8-ae7b-4650-d753-b75d4500af61"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eWVL_6WevrqD","executionInfo":{"status":"ok","timestamp":1694099076975,"user_tz":-120,"elapsed":10258,"user":{"displayName":"Joseph","userId":"02183415066300269879"}},"outputId":"35a209af-3129-434a-e9a4-6f006228d3a1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1\n"]}]},{"cell_type":"code","source":["!pip install faiss-gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ImQ_W4ZCIIKM","executionInfo":{"status":"ok","timestamp":1694099084935,"user_tz":-120,"elapsed":7969,"user":{"displayName":"Joseph","userId":"02183415066300269879"}},"outputId":"0ea8a451-c692-42b5-9ca7-c8fc0884105b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faiss-gpu\n","  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-gpu\n","Successfully installed faiss-gpu-1.7.2\n"]}]},{"cell_type":"markdown","source":["### CODER embed entity"],"metadata":{"id":"z791AgO8GXes"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModel\n","\n","coder_tokenizer = AutoTokenizer.from_pretrained(\"GanjinZero/coder_eng_pp\")\n","coder_model = AutoModel.from_pretrained(\"GanjinZero/coder_eng_pp\").to(device)\n","coder_model.output_hidden_states = False"],"metadata":{"id":"3yX3Zl35Gd0V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 160\n","\n","# Best CODER results are with [CLS] representations and normalization (default)\n","def get_bert_embed(phrase_list, model, tokenizer, normalize=True, summary_method=\"CLS\"):\n","\n","  # TOKENIZATION\n","  input_ids = []\n","  for phrase in phrase_list:\n","    # (1) Tokenize the sentence.\n","    # (2) Prepend the `[CLS]` token to the start.\n","    # (3) Append the `[SEP]` token to the end.\n","    # (4) Map tokens to their IDs.\n","    # (5) Pad or truncate the sentence to `max_length`\n","    # (6) Create attention masks for [PAD] tokens.\n","    input_ids.append(tokenizer(\n","        phrase,\n","        max_length=32, # UMLS terms are short\n","        add_special_tokens=True,\n","        truncation=True,\n","        pad_to_max_length=True)['input_ids'])\n","\n","  # INFERENCE MODE ON\n","  model.eval()\n","\n","  # COMPUTE EMBEDDINGS ACCORDING TO THE SPECIFIED BATCH-SIZE\n","  # (e.g., max_length=32, batch_size=64 --> 2 phrase embeddings at a time)\n","  count = len(input_ids) # n total tokens\n","  now_count = 0\n","  with torch.no_grad():\n","    while now_count < count:\n","      batch_input_gpu = torch.LongTensor(input_ids[\n","          now_count:min(now_count + batch_size, count)]).to(device)\n","      if summary_method == \"CLS\":\n","        embed = model(batch_input_gpu)[1]\n","      if summary_method == \"MEAN\":\n","        embed = torch.mean(model(batch_input_gpu)[0], dim=1)\n","      if normalize:\n","        embed_norm = torch.norm(\n","            embed, p=2, dim=1, keepdim=True).clamp(min=1e-12)\n","        embed = embed / embed_norm\n","      # Move embedding on CPU and convert it to a numpy array\n","      embed_np = embed.cpu().detach().numpy()\n","      # Update indeces for batch processing\n","      if now_count == 0:\n","        output = embed_np\n","      else:\n","        output = np.concatenate((output, embed_np), axis=0)\n","      now_count = min(now_count + batch_size, count)\n","  return output"],"metadata":{"id":"gP-Hl1vRHpuL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### save result"],"metadata":{"id":"6-lA2kwFHuOd"}},{"cell_type":"code","source":["with open(path + \"data/build_tri/build_entities.json\", \"r\") as json_file:\n","    build_entities = json.load(json_file)\n","    entities = list(build_entities.values())"],"metadata":{"id":"FEauUTmkHrjb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["entities_feat = get_bert_embed(entities, coder_model, coder_tokenizer)"],"metadata":{"id":"9WrTHWBfHxJZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed_file = '/data/coder_embed_entity.h5'\n","#write\n","with h5py.File(path + embed_file, 'w') as hf:\n","    hf.create_dataset('entity', data=entities.to_numpy())\n","    hf.create_dataset('embedding', data=np.array(entities_feat, dtype=np.float64))"],"metadata":{"id":"x5lzdkneHyl9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Cluster"],"metadata":{"id":"PO76HH60GeWS"}},{"cell_type":"code","source":["with h5py.File(path + embed_file, 'r') as hf:\n","    # read\n","    strings_data = hf['entity'][:]\n","    arrays_data = hf['embedding'][:]"],"metadata":{"id":"M9TugtyFH4eE","executionInfo":{"status":"ok","timestamp":1694099135257,"user_tz":-120,"elapsed":17603,"user":{"displayName":"Joseph","userId":"02183415066300269879"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import faiss\n","import matplotlib.pyplot as plt\n","\n","class FaissKMeans:\n","    def __init__(self, n_clusters=8, n_iter=20, n_redo=1):\n","        self.n_clusters = n_clusters\n","        self.n_iter = n_iter\n","        self.n_redo = n_redo\n","        self.kmeans = None\n","        self.cluster_centers_ = None\n","        self.inertia_ = None\n","\n","    def fit(self, X):\n","        self.kmeans = faiss.Kmeans(d=X.shape[1],\n","                                   k=self.n_clusters,\n","                                   niter=self.n_iter,\n","                                   nredo=self.n_redo, gpu=True)\n","        self.kmeans.train(X.astype(np.float32))\n","        self.cluster_centers_ = self.kmeans.centroids\n","        self.inertia_ = self.kmeans.obj[-1]\n","    def predict(self, X):\n","        return self.kmeans.index.search(X.astype(np.float32), 1)[1]\n","\n","def get_k(x):\n","    return len(arrays_data) // x"],"metadata":{"id":"S2T1g2tQIUF7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#find best K\n","\n","# define a cluster contains how many nodes\n","Q = [20, 15, 10, 5]\n","\n","k_values = [get_k(x) for x in Q]\n","niter_values = [5, 10, 20, 50, 100]\n","\n","best_k = None\n","best_niter = None\n","best_model = None\n","\n","best_inertia = float(\"inf\")\n","inertia_results = []\n","\n","for k in k_values:\n","    for niter in niter_values:\n","        f_cluster = FaissKMeans(k, niter)\n","        f_cluster.fit(arrays_data)\n","\n","        inertia = f_cluster.inertia_\n","        inertia_results.append(inertia)\n","\n","        print(f\"clusters: {k}\")\n","        print(f\"niter: {niter}\")\n","        print(f\"inertia: {inertia}\")\n","\n","        if inertia < best_inertia:\n","            best_inertia = inertia\n","            best_niter = niter\n","            best_k = k\n","            best_model = f_cluster.kmeans"],"metadata":{"id":"yJ3CTjOEIXPC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot\n","plt.plot(range(len(inertia_results)), inertia_results, marker='o')\n","plt.xlabel('Parameter Combination')\n","plt.ylabel('Inertia')\n","plt.title('Inertia for Different Parameter Combinations')\n","plt.xticks(range(len(inertia_results)), [f'({k},{niter})' for k in k_values for niter in niter_values], rotation=45)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"91rwsVOxIZkq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(best_k)\n","print(best_niter)"],"metadata":{"id":"gBiIh_YWIgWp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Improve stability\n","\n","nredo_values = [1, 5, 10, 15]\n","best_nredo = None\n","\n","for nredo in nredo_values:\n","\n","    f_cluster = FaissKMeans(best_k, best_niter, nredo)\n","    f_cluster.fit(arrays_data)\n","\n","    inertia = f_cluster.inertia_\n","    inertia_results.append(inertia)\n","\n","    print(f\"nredo: {nredo}\")\n","    print(f\"inertia: {inertia}\")\n","\n","    if inertia < best_inertia:\n","        best_inertia = inertia\n","        best_nredo = nredo\n","        best_model = f_cluster.kmeans"],"metadata":{"id":"sjlpN1CeIg5n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot\n","plt.plot(nredo_values, inertia_results[-4:], marker='o')\n","\n","plt.xlabel('nredo')\n","plt.ylabel('Inertia')\n","plt.title('Inertia for Different nredo Values')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"YVJMAQ0EIiet"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build a FAISS index\n","dim = arrays_data.shape[1]\n","res = faiss.StandardGpuResources()\n","index = faiss.GpuIndexFlatL2(res, dim)\n","\n","index.add(arrays_data.astype('float32'))"],"metadata":{"id":"Sa8Zc6f7Ilwo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#search for the cluster group and get the clusters\n","_, I = index.search (best_model.centroids, 5)"],"metadata":{"id":"upUzk47lIrJG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from index, group the string cluster\n","centroids = []\n","clusters = []\n","\n","for i, v in enumerate(I):\n","    #centroid = best_model.centroids[i]\n","    centroid = strings_data[v[0]]\n","    cluster = [strings_data[x] for x in v]\n","\n","    centroids.append(centroid)\n","    clusters.append(cluster)"],"metadata":{"id":"lvmvMcf8ItDC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save to h5\n","cluster_file = '/data/faiss_clusters.h5'\n","\n","with h5py.File(path + hdf5_file, 'w') as hf:\n","    hf.create_dataset('centroid', data=centroids)\n","    hf.create_dataset('cluster', data=clusters)"],"metadata":{"id":"KE_5ZPz_Iu3S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### make it as memory formate   \n","\n","cluster flatten"],"metadata":{"id":"seHhKKyCJm6p"}},{"cell_type":"code","source":["with h5py.File(path + cluster_file, 'r') as hf:\n","    # read\n","    centroids = hf['centroid'][:]\n","    clusters = hf['cluster'][:]"],"metadata":{"id":"CB42ov4vJ3uz","executionInfo":{"status":"ok","timestamp":1694099206809,"user_tz":-120,"elapsed":5821,"user":{"displayName":"Joseph","userId":"02183415066300269879"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["clusters = [\"related entities: \" + \", \".join([e.decode() for e in c]) for c in clusters]"],"metadata":{"id":"Y13R2bP9LmV6","executionInfo":{"status":"ok","timestamp":1694099392504,"user_tz":-120,"elapsed":395,"user":{"displayName":"Joseph","userId":"02183415066300269879"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["c_df = pd.DataFrame(columns=['centroid', 'cluster'])\n","\n","c_df['centroid'] = centroids\n","c_df['cluster'] = clusters\n","\n","c_df.to_csv(os.path.join(path,'data/memories/clusters_memory.csv'), index_label=False)"],"metadata":{"id":"9HVxuSq-MZt_","executionInfo":{"status":"ok","timestamp":1694099525635,"user_tz":-120,"elapsed":406,"user":{"displayName":"Joseph","userId":"02183415066300269879"}}},"execution_count":15,"outputs":[]}]}